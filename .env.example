LLM_PROVIDER=local

# Local OpenAI-compatible server (LM Studio, Ollama proxy, vLLM, etc.)
LLM_URL=http://127.0.0.1:1234/v1
LLM_MODEL=qwen2.5-7b-instruct
LLM_API_KEY=lm-studio
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=800
GIT_TIMEOUT_SEC=180
GIT_CLONE_RETRIES=3
GIT_CLONE_RETRY_DELAY_SEC=2

# Cloud providers (optional fallbacks)
GEMINI_API_KEY=
GEMINI_MODEL=gemini-2.0-flash
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# LangSmith / LangChain tracing
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=automation-auditor

LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=automation-auditor

LANGSMITH_TRACE_URL=
